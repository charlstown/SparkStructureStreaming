{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA SYSTEMAS DISTRIBUIDOS III\n",
    "**KAFKA STREAMING + SPARK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descripción:** este notebook va dirigido a la preparación y limpieza de los datos elegidos para la práctica.\n",
    "\n",
    "**Datos:** se trata de una base de datos con 3 tablas que contienen información sobre los accidentes de Reino Unido desde 2005 hasta 2014. Estos datos pueden obtenerse a través de este enlace: https://www.kaggle.com/benoit72/uk-accidents-10-years-history-with-many-variables\n",
    "\n",
    "**Miembros del equipo:** Verónica Gómez, Carlos Grande y Pablo Olmos\n",
    "\n",
    "**GitHub URL:** https://github.com/Akinorev/dataScienceGitR2/tree/master/03_DataScience_III/sd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import sys, os\n",
    "\n",
    "# spark libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from operator import add\n",
    "from operator import sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../02_scripts')\n",
    "import myKafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contexto y entorno de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS =  --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.5 pyspark-shell \n",
      "\n",
      "JAVA_HOME =  /usr/lib/jvm/java-1.8.0-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "packages = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.5\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")\n",
    "\n",
    "print(\"PYSPARK_SUBMIT_ARGS = \",os.environ[\"PYSPARK_SUBMIT_ARGS\"],\"\\n\")\n",
    "print(\"JAVA_HOME = \", os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext object (low level)\n",
    "sc = SparkContext(appName=\"PythonStreamingOccupancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every 5 seconds grab the data from the kafka stream. \n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Métodos Auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parseador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parseSample(line):\n",
    "  s = line.replace('\"','').split(\",\")\n",
    "  print(s)\n",
    "  try:\n",
    "      return [{\"rowId\": int(s[0]),\n",
    "               \"date\": datetime.strptime(s[1], \"%Y-%m-%d %H:%M:%S\"),\n",
    "               \"temperature\": float(s[2]), \n",
    "               \"humidity\": float(s[3]), \n",
    "               \"light\": float(s[4]),\n",
    "               \"co2\": float(s[5]), \n",
    "               \"humidityRatio\": float(s[6]),\n",
    "               \"occupancy\": bool(s[7][0])}]\n",
    "  except Exception as err:\n",
    "      print(\"Wrong sample format (%s): \" % line)\n",
    "      print(err)\n",
    "      return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura el endpoint para localizar el broker de Kafka\n",
    "kafkaBrokerIPPort = \"127.0.0.1:9092\"\n",
    "\n",
    "# Productor simple (Singleton!)\n",
    "# from kafka import KafkaProducer\n",
    "import kafka\n",
    "class KafkaProducerWrapper(object):\n",
    "  producer = None\n",
    "  @staticmethod\n",
    "  def getProducer(brokerList):\n",
    "    if KafkaProducerWrapper.producer != None:\n",
    "      return KafkaProducerWrapper.producer\n",
    "    else:\n",
    "      KafkaProducerWrapper.producer = kafka.KafkaProducer(bootstrap_servers=brokerList,\n",
    "                                                          key_serializer=str.encode,\n",
    "                                                          value_serializer=str.encode)\n",
    "      return KafkaProducerWrapper.producer\n",
    "\n",
    "# Envía resultados a Kafka! (salida)  \n",
    "def sendResults(itr):\n",
    "  prod = KafkaProducerWrapper.getProducer([kafkaBrokerIPPort])\n",
    "  for m in itr:\n",
    "    prod.send(\"metrics\", key=m[0], value=m[0]+\",\"+str(m[1]))\n",
    "  prod.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lectura de datos en Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka: Lectura de datos\n",
    "kafka_port = '0.0.0.0:9092'\n",
    "kafkaParams = {\"metadata.broker.list\": kafka_port}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [\"test\"], kafkaParams)\n",
    "stream = stream.map(lambda o: str(o[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Promedio de valores\n",
    "Calcular el promedio de valores de temperatura, humedad relativa y concentración de CO2 para cada micro-batch, y el promedio de dichos valores desde el arranque de la aplicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = stream.flatMap(parseSample)\n",
    "samples = samples.flatMap(lambda x: x.items())\n",
    "\n",
    "# -----------------------------------\n",
    "#        PROMEDIO POR BATCH\n",
    "# -----------------------------------\n",
    "\n",
    "avgValuesInBatch = samples \\\n",
    "                   .filter(lambda k: k[0] in (\"temperature\",\"humidity\",\"co2\")) \\\n",
    "                   .mapValues(lambda v: (v,1)) \\\n",
    "                   .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])) \\\n",
    "                   .mapValues(lambda v: v[0]/v[1]) \\\n",
    "                   .repartition(1) \\\n",
    "                   .glom() \\\n",
    "                   .map(lambda arr: (\"Average values in batch\", arr))\n",
    "\n",
    "avgValuesInBatch.foreachRDD(lambda rdd: rdd.foreachPartition(sendResults))\n",
    "\n",
    "# -----------------------------------\n",
    "#        PROMEDIO EN TOTAL\n",
    "# -----------------------------------\n",
    "\n",
    "def update_func(new_val, last_val):\n",
    "    # NEW_VAL FORMAT: [(236, 6)] \n",
    "    # in which the tuple indicates the sum of the values in the batch and the count in the batch\n",
    "    # LAST_VAL FORMAT: (1401,12)\n",
    "    # in which the tuple indicates the sum of all the past values and the count of all past records\n",
    "    new_val = new_val[0]\n",
    "    if last_val != None:\n",
    "        totalSum = new_val[0] + last_val[0]\n",
    "        totalCount = new_val[1] + last_val[1]\n",
    "    else:\n",
    "        totalSum = new_val[0]\n",
    "        totalCount = new_val[1]\n",
    "    return (totalSum, totalCount)\n",
    "   \n",
    "\n",
    "avgValuesTotal = samples \\\n",
    "                 .filter(lambda k: k[0] in (\"temperature\",\"humidity\",\"co2\")) \\\n",
    "                 .mapValues(lambda v: (v,1)) \\\n",
    "                 .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])) \\\n",
    "                 .updateStateByKey(update_func) \\\n",
    "                 .mapValues(lambda v: v[0]/v[1]) \\\n",
    "                 .repartition(1) \\\n",
    "                 .glom() \\\n",
    "                 .map(lambda arr: (\"Average values in total\", arr))\n",
    "                     \n",
    "\n",
    "avgValuesTotal.foreachRDD(lambda rdd: rdd.foreachPartition(sendResults))\n",
    "\n",
    "sc.setCheckpointDir(\"../01_data/checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "# ssc.awaitTerminationOrTimeout(10) # Espera 10 segs. antes de acabar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
