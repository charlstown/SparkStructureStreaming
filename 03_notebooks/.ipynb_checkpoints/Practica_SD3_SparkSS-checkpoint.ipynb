{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÁCTICA SYSTEMAS DISTRIBUIDOS III <a class=\"anchor\" id=\"back\">\n",
    "<font color='#fb7813'>**KAFKA STREAMING + SPARK**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descripción:** este notebook tiene como objetivo leer los datos proporcionados en CSV mediante un productor que envíe los datos a una cola Kafka de nombre *test* con un retardovariable entre muestras insertadas de entre 0.6 y 1.3 segundos. Posteriormente, usando la API de *Spark Structured Streaming* con un intervalo de actualizacion de micro-batches (triggers) de 5 segundos se pide realizar 3 consultas.\n",
    "\n",
    "**Datos:** Los datos contienen información de un experimento para intentar predecir los tiempos de ocupación de una estancia, en función de los valores tomados por variables ambientales.los datos proporcionados están en formato CSV. El archivo occupancy_data.csv contiene 8.143 entradas, una para cada muestra recogida, a intervalos aproximadamente regulares de 1 minuto entre muestras. \n",
    "\n",
    "**Miembros del equipo:** Verónica Gómez, Carlos Grande y Pablo Olmos\n",
    "\n",
    "**GitHub URL:** https://github.com/Akinorev/dataScienceGitR2/tree/master/03_DataScience_III/sd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "* [0. Instrucciones](#readme)\n",
    "* [1. Contexto y entorno de Spark](#context)\n",
    "* [2. Lectura de datos en batch](#batch)\n",
    "* [3. Conexión con Kafka](#kafka)\n",
    "* [4. Definición del Stream](#stream)\n",
    "* [5. Queries](#queries)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, TimestampType, DoubleType,\\\n",
    "    IntegerType, StringType, ArrayType, BooleanType\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../02_scripts')\n",
    "import myKafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Contexto y entorno de Spark <a class=\"anchor\" id=\"context\">\n",
    "[<font color='#fb7813'>**\\>\\> Volver arriba**</font>](#back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS =  --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 pyspark-shell \n",
      "\n",
      "JAVA_HOME =  /usr/lib/jvm/java-1.8.0-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "packages = \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages {0} pyspark-shell\".format(packages)\n",
    ")\n",
    "\n",
    "print(\"PYSPARK_SUBMIT_ARGS = \",os.environ[\"PYSPARK_SUBMIT_ARGS\"],\"\\n\")\n",
    "print(\"JAVA_HOME = \", os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Lenovo:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PracticaSD3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff167d1d990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create SparkContext object (low level)\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PracticaSD3\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lectura de datos en Batch <a class=\"anchor\" id=\"batch\">\n",
    "[<font color='#fb7813'>**\\>\\> Volver arriba**</font>](#back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creación del esquema y muestra de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_schema = StructType([\n",
    "    StructField(\"row\", IntegerType(), True),\n",
    "    StructField(\"date\", TimestampType(), True), \n",
    "    StructField(\"Temperature\", DoubleType(), True),\n",
    "    StructField(\"Humidity\", DoubleType(), True),\n",
    "    StructField(\"Light\", DoubleType(), True),\n",
    "    StructField(\"CO2\", DoubleType(), True),\n",
    "    StructField(\"HumidityRatio\", DoubleType(), True),\n",
    "    StructField(\"Occupancy\", IntegerType(), True) \n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = spark.read.option(\"header\", True).csv('../01_data/occupancy_data.csv', \n",
    "                                                  schema=custom_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+--------+-----+------+-------------------+---------+\n",
      "|row|               date|Temperature|Humidity|Light|   CO2|      HumidityRatio|Occupancy|\n",
      "+---+-------------------+-----------+--------+-----+------+-------------------+---------+\n",
      "|  1|2015-02-04 17:51:00|      23.18|  27.272|426.0|721.25|0.00479298817650529|        1|\n",
      "|  2|2015-02-04 17:51:59|      23.15| 27.2675|429.5| 714.0|0.00478344094931065|        1|\n",
      "|  3|2015-02-04 17:53:00|      23.15|  27.245|426.0| 713.5|0.00477946352442199|        1|\n",
      "|  4|2015-02-04 17:54:00|      23.15|    27.2|426.0|708.25|0.00477150882608175|        1|\n",
      "|  5|2015-02-04 17:55:00|       23.1|    27.2|426.0| 704.5|0.00475699293331518|        1|\n",
      "+---+-------------------+-----------+--------+-----+------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comprobación datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+--------+-----+---+-------------+---------+\n",
      "|row|date|Temperature|Humidity|Light|CO2|HumidityRatio|Occupancy|\n",
      "+---+----+-----------+--------+-----+---+-------------+---------+\n",
      "|  0|   0|          0|       0|    0|  0|            0|        0|\n",
      "+---+----+-----------+--------+-----+---+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = batch.columns\n",
    "batch.select([fn.count(fn.when(fn.col(colmn).isNull(), colmn))\n",
    "              .alias(colmn) for colmn in columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conexión con Kafka <a class=\"anchor\" id=\"kafka\">\n",
    "[<font color='#fb7813'>**\\>\\> Volver arriba**</font>](#back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_stream = spark\\\n",
    "    .readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"test\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definición del Stream <a class=\"anchor\" id=\"stream\">\n",
    "[<font color='#fb7813'>**\\>\\> Volver arriba**</font>](#back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = read_stream\\\n",
    "    .withColumn('array_value',\n",
    "                fn.split(fn.col('value').cast(StringType()), ',')\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos OrderedDict para leer values en orden\n",
    "schema = OrderedDict()\n",
    "schema[\"row\"] = IntegerType()\n",
    "schema[\"date\"] = TimestampType()\n",
    "schema[\"Temperature\"] = DoubleType()\n",
    "schema[\"Humidity\"] = DoubleType()\n",
    "schema[\"Light\"] = DoubleType()\n",
    "schema[\"CO2\"] = DoubleType()\n",
    "schema[\"HumidityRatio\"] = DoubleType()\n",
    "schema[\"Occupancy\"] = IntegerType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream_schema = myKafka.apply_schema(df_stream, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = (df_stream_schema\n",
    "                .select(*columns)\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"test_query\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode('append')\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "|row|               date|Temperature|        Humidity|Light|             CO2|      HumidityRatio|Occupancy|\n",
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "| 63|2015-02-04 18:53:00|      22.39|           27.39|  0.0|           620.0| 0.0045870824294015|        0|\n",
      "| 64|2015-02-04 18:53:59|      22.29|           27.39|  0.0|           621.5|0.00455905614852278|        0|\n",
      "| 65|2015-02-04 18:55:00|      22.29|27.3566666666667|  0.0|616.333333333333|0.00455346720904263|        0|\n",
      "| 66|2015-02-04 18:55:59|      22.29|           27.34|  0.0|           609.0|0.00455067277669262|        0|\n",
      "| 67|2015-02-04 18:56:59|      22.29|           27.39|  0.0|           614.0|0.00455905614852278|        0|\n",
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep(11)\n",
    "df = spark.sql(\"SELECT * FROM test_query\")\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Queries <a class=\"anchor\" id=\"queries\">\n",
    "[<font color='#fb7813'>**\\>\\> Volver arriba**</font>](#back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Promedio de valores\n",
    "Calcular el promedio de valores de temperatura, humedad relativa y concentración de CO2 para cada micro-batch, y el promedio de dichos valores desde el arranque de la aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por Microbatch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Temperature\", \"Humidity\", \"CO2\"]\n",
    "query_1a = (df_stream_schema\n",
    "             .select(*[fn.avg(col).alias(\"avg_\" + col) for col in columns])\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"query1a\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode(\"update\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+\n",
      "|   avg_Temperature|      avg_Humidity|          avg_CO2|\n",
      "+------------------+------------------+-----------------+\n",
      "|              null|              null|             null|\n",
      "|          22.21125|27.278750000000002|           593.75|\n",
      "|22.199499999999997|27.262999999999998|592.2333333333333|\n",
      "+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(11)\n",
    "df = spark.sql(\"SELECT * FROM query1a\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1a.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desde memoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1b = (df_stream_schema\n",
    "             .select(*[fn.avg(col).alias(\"avg_\" + col) for col in columns])\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"query1b\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .outputMode('update')\n",
    "    .option(\"checkpointLocation\", \"../01_data/checkpoints\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+\n",
      "|   avg_Temperature|      avg_Humidity|           avg_CO2|\n",
      "+------------------+------------------+------------------+\n",
      "|21.332408774087664|25.233808323083277| 486.3254407544077|\n",
      "|  21.3330937755937|25.236100941850985|486.40980753480767|\n",
      "|21.336493691493615|25.247480056980102| 486.7992470492472|\n",
      "|21.339751213592155|25.258721076051824| 487.1762742718448|\n",
      "+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(11)\n",
    "df = spark.sql(\"SELECT * FROM query1b\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1b.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Promedio luminosidad\n",
    "Calcular el promedio de luminosidad en la estancia en ventanas deslizantes de tamaño 45 segundos, con un valor de deslizamiento de 15 segundos entre ventanas consecutivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_window = (df_stream_schema\n",
    "                .withWatermark(\"date\", \"45 seconds\")\n",
    "                .groupBy(fn.window(\"date\",\n",
    "                                \"45 seconds\",\n",
    "                                \"15 seconds\")\n",
    "                        )\n",
    "                .agg(fn.avg(\"Light\").alias(\"avg_light\"))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = (df_window\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"query2\")\n",
    "    .trigger(processingTime='5 seconds')\n",
    "    .outputMode('append')\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+\n",
      "|window                                    |avg_light|\n",
      "+------------------------------------------+---------+\n",
      "|[2015-02-05 14:42:30, 2015-02-05 14:43:15]|515.0    |\n",
      "|[2015-02-05 14:42:00, 2015-02-05 14:42:45]|525.5    |\n",
      "|[2015-02-05 14:41:45, 2015-02-05 14:42:30]|525.5    |\n",
      "|[2015-02-05 14:41:30, 2015-02-05 14:42:15]|525.5    |\n",
      "|[2015-02-05 14:41:00, 2015-02-05 14:41:45]|479.0    |\n",
      "+------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM query2\")\n",
    "df.orderBy(\"window\", ascending=False).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Intervalo de muestras\n",
    "Examinando los datos, podemos apreciar que el intervalo entre muestras originales no es exactamente de 1 minuto en muchos casos. Calcular el número de parejas de muestras consecutivas en cada micro-batch entre las cuales el intervalo de separación no es exactamente de\n",
    "1 minuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "|        |   query1a|       true|\n",
      "|        |   query1b|       true|\n",
      "|        |    query2|       true|\n",
      "|        |test_query|       true|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
